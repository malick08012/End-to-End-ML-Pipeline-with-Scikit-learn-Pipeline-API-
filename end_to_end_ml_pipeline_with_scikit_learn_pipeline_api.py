# -*- coding: utf-8 -*-
"""End-to-End ML Pipeline with Scikit-learn Pipeline API

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bm3QGpfH0XE2R92yb1gC1J8fRPCaGoWA

#Connect our google drive in google collab
"""

from google.colab import drive
drive.mount('/content/drive')

"""#Import Libraries
Required libraries for this task
"""

#STEP 1: Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
import joblib

"""#Upload Dataset
In this step we upload our dataset manually in our google drive in CSV format
"""

#STEP 2: Upload the Telco Churn Dataset
from google.colab import files
uploaded = files.upload()

"""#Load Dataset
Reads the Telco Churn CSV file into a pandas DataFrame so we can work with it easily.
"""

df = pd.read_csv('/content/WA_Fn-UseC_-Telco-Customer-Churn.csv')

# View first few rows
df.head()

"""#Data Cleaning"""

#STEP 3: Data Cleaning
# Drop customerID (not useful for prediction)
df.drop('customerID', axis=1, inplace=True)

"""TotalCharges had some empty strings, so we converted it to numbers."""

# Convert TotalCharges to numeric (some empty strings cause issues)
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

"""Missing rows were dropped to ensure clean training.


"""

# Drop rows with missing values
df.dropna(inplace=True)

"""Churn (our target) was converted to 1 (churned) and 0 (did not churn) for ML."""

# Convert target column Churn from Yes/No → 1/0
df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})

# Preview cleaned data
df.head()

"""#Result:
Clean, numerical, model-ready dataset.

#Split Features and Target
We separate input features X and the target y.

Split into training and test sets (80% training, 20% testing) to evaluate model performance later.
"""

#STEP 4: Split Features and Target
X = df.drop('Churn', axis=1)
y = df['Churn']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)

"""#Preprocessing Pipeline"""

# STEP 5: Preprocessing Pipeline
# Identify column types
numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns
categorical_cols = X.select_dtypes(include=['object']).columns

"""Num pipeline: Handles missing values + scales numeric data"""

# Numerical pipeline: impute and scale
num_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

"""Categorical pipeline: Fills missing + encodes categorical data"""

# Categorical pipeline: impute and encode
cat_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])

"""ColumnTransformer applies both pipelines to their respective columns."""

# Combine pipelines using ColumnTransformer
preprocessor = ColumnTransformer(transformers=[
    ('num', num_pipeline, numerical_cols),
    ('cat', cat_pipeline, categorical_cols)
])

"""#Result:
All data is automatically preprocessed inside a pipeline — no manual steps required when making predictions.

#Create Full Pipelines with Models
Combines preprocessing + model into one unit.

So we can treat the whole ML workflow as a single object.
"""

# Logistic Regression pipeline
pipe_lr = Pipeline(steps=[
    ('preprocessing', preprocessor),
    ('classifier', LogisticRegression(max_iter=1000))
])

# Random Forest pipeline
pipe_rf = Pipeline(steps=[
    ('preprocessing', preprocessor),
    ('classifier', RandomForestClassifier())
])

"""#Hyperparameter Tuning with GridSearchCV
Automatically tests different parameter values and selects the best combination using cross-validation (cv=5).

Improves model performance.
"""

# Logistic Regression params
param_grid_lr = {
    'classifier__C': [0.01, 0.1, 1, 10]
}

# Random Forest params
param_grid_rf = {
    'classifier__n_estimators': [100, 200],
    'classifier__max_depth': [None, 10, 20]
}

# GridSearch for LR
grid_lr = GridSearchCV(pipe_lr, param_grid_lr, cv=5, scoring='accuracy')
grid_lr.fit(X_train, y_train)

# GridSearch for RF
grid_rf = GridSearchCV(pipe_rf, param_grid_rf, cv=5, scoring='accuracy')
grid_rf.fit(X_train, y_train)

"""#Evaluate Best Models
We use the test set to check how well the model performs on unseen data.

#Logistic Regression
"""

# Predict on test set
pred_lr = grid_lr.predict(X_test)

# Accuracy
print(" Logistic Regression Accuracy:", accuracy_score(y_test, pred_lr))

# Classification reports
print("\n Logistic Regression Report:\n", classification_report(y_test, pred_lr))

"""#Random Forest"""

# Predict on test set
pred_rf = grid_rf.predict(X_test)
# Accuracy
print(" Random Forest Accuracy:", accuracy_score(y_test, pred_rf))
# Classification reports
print("\n Random Forest Report:\n", classification_report(y_test, pred_rf))

"""#Result
After checking Model Performance ( Random Forest ) Perform well with Accuracy of 83% as compared to ( Logistic Regression ) with accuracy of 80%.

#Export the Best Pipeline using joblib
Saves the entire pipeline (preprocessing + classifier + best parameters) to a file.

So you can reuse it without retraining.
"""

# Save the better model (RF here)
joblib.dump(grid_rf.best_estimator_, 'telco_churn_pipeline.pkl')

"""#Load and Reuse the Pipeline
We can now load the model anytime and use .predict() on new customer data, without redoing training or preprocessing.
"""

# Load model
model = joblib.load('telco_churn_pipeline.pkl')

# Predict again (just to test reusability)
predictions = model.predict(X_test)
print("✅ Reused model accuracy:", accuracy_score(y_test, predictions))

"""#Result:
Reusable, deployable, production-ready ML model.

# Final Insights
1. Model Performance:

Random Forest outperformed Logistic Regression with an accuracy of approximately 83%, making it the best choice for predicting customer churn.

2. Key Factors Influencing Churn:

Month-to-month contracts

Higher monthly charges

No online security or tech support

Shorter tenure (new customers)
are more likely to churn.

3. Reusability & Deployment Readiness:

The full machine learning workflow — from preprocessing to model prediction — was wrapped in a single reusable pipeline using scikit-learn’s Pipeline and ColumnTransformer.

The model was exported using joblib, making it ready for:

Deployment in web or cloud apps

Scheduled batch predictions

Reuse in future projects without retraining

4. Learning and Professional Growth:

Gained hands-on experience with:

Building production-ready ML pipelines

Using GridSearchCV for hyperparameter tuning

Saving and loading models for real-world usage

5. Business Value:

This solution can help telecom companies:

Proactively identify at-risk customers

Personalize retention strategies

Reduce churn rate and improve customer lifetime value
"""